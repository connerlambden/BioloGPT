{"nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "### Load and Prepare Experimental Data\nLoad experimental mRNA expression data for a two-state promoter model using NumPy and PyTorch."}, {"cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null, "source": "import numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n\n# Load promoter experimental data (replace path with actual data file)\ndata = np.load('science_data_lacUD5.npy')\ndata_tensor = torch.tensor(data, dtype=torch.float32)\n\n# Display basic statistics\nprint('Data mean:', data_tensor.mean().item(), 'Std:', data_tensor.std().item())"}, {"cell_type": "markdown", "metadata": {}, "source": "### Define a Two-State Promoter Model\nDefine a simple neural network model to approximate the kinetic parameters using differentiable programming."}, {"cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null, "source": "class TwoStatePromoterModel(torch.nn.Module):\n    def __init__(self):\n        super(TwoStatePromoterModel, self).__init__()\n        self.k_on = torch.nn.Parameter(torch.tensor(0.5))\n        self.r = torch.nn.Parameter(torch.tensor(1.0))\n        self.gamma = torch.nn.Parameter(torch.tensor(0.5))\n    def forward(self, x):\n        # Simplified model: expected mRNA level\n        return self.k_on * self.r / self.gamma\n\nmodel = TwoStatePromoterModel()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.MSELoss()\n\n# Dummy training loop for demonstration\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    output = model(data_tensor)\n    loss = criterion(output, data_tensor.mean())\n    loss.backward()\n    optimizer.step()\n\nprint('Trained parameters:', model.k_on.item(), model.r.item(), model.gamma.item())\n\n# Plot result\nplt.figure(figsize=(6,4))\nplt.plot([data_tensor.mean().item()], 'ro', label='Experimental Mean')\nplt.title('Parameter Fit Using DGA Principles')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "This notebook demonstrates basic kinetic parameter fitting based on the DGA approach. More complex models and error analyses can be integrated for full experimental validation."}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n\n\n***\n### [**Evolve This Code**](https://biologpt.com/?q=Evolve%20Code%3A%20This%20code%20demonstrates%20DGA-based%20parameter%20fitting%20on%20experimental%20promoter%20data%2C%20enabling%20kinetic%20parameter%20extraction%20via%20gradient%20descent.%0A%0AExpand%20the%20model%20to%20simulate%20full%20reaction%20dynamics%20and%20validate%20against%20additional%20experimental%20datasets.%0A%0ADifferentiable%20Gillespie%20algorithm%20chemical%20kinetics%20parameter%20estimation%20synthetic%20biology%20circuits%0A%0A%23%23%23%20Load%20and%20Prepare%20Experimental%20Data%0ALoad%20experimental%20mRNA%20expression%20data%20for%20a%20two-state%20promoter%20model%20using%20NumPy%20and%20PyTorch.%0A%0Aimport%20numpy%20as%20np%0Aimport%20torch%0Aimport%20matplotlib.pyplot%20as%20plt%0A%0A%23%20Load%20promoter%20experimental%20data%20%28replace%20path%20with%20actual%20data%20file%29%0Adata%20%3D%20np.load%28%27science_data_lacUD5.npy%27%29%0Adata_tensor%20%3D%20torch.tensor%28data%2C%20dtype%3Dtorch.float32%29%0A%0A%23%20Display%20basic%20statistics%0Aprint%28%27Data%20mean%3A%27%2C%20data_tensor.mean%28%29.item%28%29%2C%20%27Std%3A%27%2C%20data_tensor.std%28%29.item%28%29%29%0A%0A%23%23%23%20Define%20a%20Two-State%20Promoter%20Model%0ADefine%20a%20simple%20neural%20network%20model%20to%20approximate%20the%20kinetic%20parameters%20using%20differentiable%20programming.%0A%0Aclass%20TwoStatePromoterModel%28torch.nn.Module%29%3A%0A%20%20%20%20def%20__init__%28self%29%3A%0A%20%20%20%20%20%20%20%20super%28TwoStatePromoterModel%2C%20self%29.__init__%28%29%0A%20%20%20%20%20%20%20%20self.k_on%20%3D%20torch.nn.Parameter%28torch.tensor%280.5%29%29%0A%20%20%20%20%20%20%20%20self.r%20%3D%20torch.nn.Parameter%28torch.tensor%281.0%29%29%0A%20%20%20%20%20%20%20%20self.gamma%20%3D%20torch.nn.Parameter%28torch.tensor%280.5%29%29%0A%20%20%20%20def%20forward%28self%2C%20x%29%3A%0A%20%20%20%20%20%20%20%20%23%20Simplified%20model%3A%20expected%20mRNA%20level%0A%20%20%20%20%20%20%20%20return%20self.k_on%20%2A%20self.r%20%2F%20self.gamma%0A%0Amodel%20%3D%20TwoStatePromoterModel%28%29%0Aoptimizer%20%3D%20torch.optim.Adam%28model.parameters%28%29%2C%20lr%3D0.01%29%0Acriterion%20%3D%20torch.nn.MSELoss%28%29%0A%0A%23%20Dummy%20training%20loop%20for%20demonstration%0Afor%20epoch%20in%20range%281000%29%3A%0A%20%20%20%20optimizer.zero_grad%28%29%0A%20%20%20%20output%20%3D%20model%28data_tensor%29%0A%20%20%20%20loss%20%3D%20criterion%28output%2C%20data_tensor.mean%28%29%29%0A%20%20%20%20loss.backward%28%29%0A%20%20%20%20optimizer.step%28%29%0A%0Aprint%28%27Trained%20parameters%3A%27%2C%20model.k_on.item%28%29%2C%20model.r.item%28%29%2C%20model.gamma.item%28%29%29%0A%0A%23%20Plot%20result%0Aplt.figure%28figsize%3D%286%2C4%29%29%0Aplt.plot%28%5Bdata_tensor.mean%28%29.item%28%29%5D%2C%20%27ro%27%2C%20label%3D%27Experimental%20Mean%27%29%0Aplt.title%28%27Parameter%20Fit%20Using%20DGA%20Principles%27%29%0Aplt.legend%28%29%0Aplt.show%28%29%0A%0AThis%20notebook%20demonstrates%20basic%20kinetic%20parameter%20fitting%20based%20on%20the%20DGA%20approach.%20More%20complex%20models%20and%20error%20analyses%20can%20be%20integrated%20for%20full%20experimental%20validation.%0A%0A)\n***\n\n### [Created with BioloGPT](https://biologpt.com/?q=Paper%20Review%3A%20A%20differentiable%20Gillespie%20algorithm%20for%20simulating%20chemical%20kinetics%2C%20parameter%20estimation%2C%20and%20designing%20synthetic%20biological%20circuits)\n[![BioloGPT Logo](https://biologpt.com/static/icons/bioinformatics_wizard.png)](https://biologpt.com/)\n***"}], "metadata": {"title": "This code demonstrates DGA-based parameter fitting on experimental promoter data, enabling kinetic parameter extraction via gradient descent.", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "pygments_lexer": "ipython3"}, "author": "BioloGPT", "creation_date": "2025-03-27", "tags": ["differentiable Gillespie algorithm", "biology", "bioinformatics"]}}