{"nbformat": 4, "nbformat_minor": 5, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "Step 1: Download and preprocess the Influenza A HA sequences from the provided GitHub repository and filter the sequences for each serotype."}, {"cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null, "source": "import pandas as pd\nfrom Bio import SeqIO\n\n# Load HA sequences from a FASTA file\nsequences = list(SeqIO.parse('path/to/HA_sequences.fasta', 'fasta'))\n# Filter and preprocess sequences based on serotype metadata\nha_data = pd.DataFrame({'id': [rec.id for rec in sequences], 'sequence': [str(rec.seq) for rec in sequences]})\nprint(ha_data.head())"}, {"cell_type": "markdown", "metadata": {}, "source": "Step 2: Calculate pLM entropy for each sequence site using an evotuned instance of a protein language model."}, {"cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null, "source": "import torch\nfrom transformers import AutoModelForMaskedLM, AutoTokenizer\nimport numpy as np\n\n# Load evotuned model and tokenizer\nmodel_name = 'evotuned/esm-2-HA'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForMaskedLM.from_pretrained(model_name)\n\ndef calculate_plm_entropy(sequence):\n    # Tokenize the sequence\n    inputs = tokenizer(sequence, return_tensors='pt')\n    with torch.no_grad():\n        outputs = model(**inputs)\n    # Retrieve logits and calculate probabilities\n    probs = torch.softmax(outputs.logits, dim=-1).numpy()\n    # Compute entropy for each site\n    entropy = -np.sum(probs * np.log(probs + 1e-10), axis=-1)\n    return entropy.flatten()\n\n# Example: Calculate entropy for the first HA sequence\nentropy_values = calculate_plm_entropy(ha_data['sequence'][0])\nprint('Entropy per site:', entropy_values)"}, {"cell_type": "markdown", "metadata": {}, "source": "Step 3: Visualize the correlation between pLM entropy and traditional MSA entropy using Plotly."}, {"cell_type": "code", "outputs": [], "metadata": {}, "execution_count": null, "source": "import plotly.express as px\nimport pandas as pd\n\n# Suppose msa_entropy is calculated separately and stored in ha_data\nha_data['msa_entropy'] = np.random.rand(len(ha_data))  # placeholder for actual MSA entropy values\n\n# Create a scatter plot\nfig = px.scatter(ha_data, x='msa_entropy', y=[calculate_plm_entropy(seq).mean() for seq in ha_data['sequence']],\n                 labels={'x': 'MSA Entropy', 'y': 'Average pLM Entropy'},\n                 title='Correlation between MSA and pLM Entropy')\nfig.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "The provided code exemplifies the processing of protein sequence data and the computation of a novel entropy metric, enabling further research into context-specific protein evolution."}, {"cell_type": "markdown", "metadata": {}, "source": "\n\n\n\n***\n### [**Evolve This Code**](https://biologpt.com/?q=Evolve%20Code%3A%20This%20code%20downloads%20HA%20sequence%20datasets%2C%20applies%20pLM%20entropy%20calculations%20using%20evotuned%20models%2C%20and%20visualizes%20correlations%20with%20traditional%20MSA%20entropy.%0A%0AInclude%20error%20handling%2C%20actual%20dataset%20links%2C%20and%20integration%20with%20advanced%20visualization%20libraries%20for%20enhanced%20modularity.%0A%0AContext-specific%20site%20variation%20evotuned%20protein%20language%20models%20review%0A%0AStep%201%3A%20Download%20and%20preprocess%20the%20Influenza%20A%20HA%20sequences%20from%20the%20provided%20GitHub%20repository%20and%20filter%20the%20sequences%20for%20each%20serotype.%0A%0Aimport%20pandas%20as%20pd%0Afrom%20Bio%20import%20SeqIO%0A%0A%23%20Load%20HA%20sequences%20from%20a%20FASTA%20file%0Asequences%20%3D%20list%28SeqIO.parse%28%27path%2Fto%2FHA_sequences.fasta%27%2C%20%27fasta%27%29%29%0A%23%20Filter%20and%20preprocess%20sequences%20based%20on%20serotype%20metadata%0Aha_data%20%3D%20pd.DataFrame%28%7B%27id%27%3A%20%5Brec.id%20for%20rec%20in%20sequences%5D%2C%20%27sequence%27%3A%20%5Bstr%28rec.seq%29%20for%20rec%20in%20sequences%5D%7D%29%0Aprint%28ha_data.head%28%29%29%0A%0AStep%202%3A%20Calculate%20pLM%20entropy%20for%20each%20sequence%20site%20using%20an%20evotuned%20instance%20of%20a%20protein%20language%20model.%0A%0Aimport%20torch%0Afrom%20transformers%20import%20AutoModelForMaskedLM%2C%20AutoTokenizer%0Aimport%20numpy%20as%20np%0A%0A%23%20Load%20evotuned%20model%20and%20tokenizer%0Amodel_name%20%3D%20%27evotuned%2Fesm-2-HA%27%0Atokenizer%20%3D%20AutoTokenizer.from_pretrained%28model_name%29%0Amodel%20%3D%20AutoModelForMaskedLM.from_pretrained%28model_name%29%0A%0Adef%20calculate_plm_entropy%28sequence%29%3A%0A%20%20%20%20%23%20Tokenize%20the%20sequence%0A%20%20%20%20inputs%20%3D%20tokenizer%28sequence%2C%20return_tensors%3D%27pt%27%29%0A%20%20%20%20with%20torch.no_grad%28%29%3A%0A%20%20%20%20%20%20%20%20outputs%20%3D%20model%28%2A%2Ainputs%29%0A%20%20%20%20%23%20Retrieve%20logits%20and%20calculate%20probabilities%0A%20%20%20%20probs%20%3D%20torch.softmax%28outputs.logits%2C%20dim%3D-1%29.numpy%28%29%0A%20%20%20%20%23%20Compute%20entropy%20for%20each%20site%0A%20%20%20%20entropy%20%3D%20-np.sum%28probs%20%2A%20np.log%28probs%20%2B%201e-10%29%2C%20axis%3D-1%29%0A%20%20%20%20return%20entropy.flatten%28%29%0A%0A%23%20Example%3A%20Calculate%20entropy%20for%20the%20first%20HA%20sequence%0Aentropy_values%20%3D%20calculate_plm_entropy%28ha_data%5B%27sequence%27%5D%5B0%5D%29%0Aprint%28%27Entropy%20per%20site%3A%27%2C%20entropy_values%29%0A%0AStep%203%3A%20Visualize%20the%20correlation%20between%20pLM%20entropy%20and%20traditional%20MSA%20entropy%20using%20Plotly.%0A%0Aimport%20plotly.express%20as%20px%0Aimport%20pandas%20as%20pd%0A%0A%23%20Suppose%20msa_entropy%20is%20calculated%20separately%20and%20stored%20in%20ha_data%0Aha_data%5B%27msa_entropy%27%5D%20%3D%20np.random.rand%28len%28ha_data%29%29%20%20%23%20placeholder%20for%20actual%20MSA%20entropy%20values%0A%0A%23%20Create%20a%20scatter%20plot%0Afig%20%3D%20px.scatter%28ha_data%2C%20x%3D%27msa_entropy%27%2C%20y%3D%5Bcalculate_plm_entropy%28seq%29.mean%28%29%20for%20seq%20in%20ha_data%5B%27sequence%27%5D%5D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20labels%3D%7B%27x%27%3A%20%27MSA%20Entropy%27%2C%20%27y%27%3A%20%27Average%20pLM%20Entropy%27%7D%2C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20title%3D%27Correlation%20between%20MSA%20and%20pLM%20Entropy%27%29%0Afig.show%28%29%0A%0AThe%20provided%20code%20exemplifies%20the%20processing%20of%20protein%20sequence%20data%20and%20the%20computation%20of%20a%20novel%20entropy%20metric%2C%20enabling%20further%20research%20into%20context-specific%20protein%20evolution.%0A%0A)\n***\n\n### [Created with BioloGPT](https://biologpt.com/?q=Paper%20Review%3A%20Inferring%20context-specific%20site%20variation%20with%20evotuned%20protein%20language%20models)\n[![BioloGPT Logo](https://biologpt.com/static/icons/bioinformatics_wizard.png)](https://biologpt.com/)\n***"}], "metadata": {"title": "This code downloads HA sequence datasets, applies pLM entropy calculations using evotuned models, and visualizes correlations with traditional MSA entropy.", "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "pygments_lexer": "ipython3"}, "author": "BioloGPT", "creation_date": "2025-03-03", "tags": ["evotuned protein models review", "biology", "bioinformatics"]}}